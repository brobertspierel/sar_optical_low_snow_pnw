# def obtain_data(bool,version,filepath): 
# 	"""Unpickles pickled snotel data from the snotel API."""
# 	if bool: 
# 		#get new data
# 		pickle_results=combine.snotel_compiler(sites_ids,state,parameter,start_date,end_date,True) #this generates a list of dataframes of all of the snotel stations that have data for a given state
# 		results=combine.pickle_opener(state,filepath)
# 		return results
# 	else: 
# 		#use pickled data 
# 		results=combine.pickle_opener(state,filepath)
# 		#print (len(results))
# 		return results
############################################################################################
############################################################################################
############################################################################################
#assign some global variables
#path = Path('/vol/v1/general_files/user_files/ben/')

#uncomment to run other things- just getting the data that we dont need right now
#sites_ids = combine.site_list(path/'oregon_snotel_sites.csv')[1] #this is getting a list of just the snotel site ids. You can grab a list slice to restrict how big results gets below. 
#######################################
#uncomment when running the full archive 
# sites_full = combine.site_list(path/'oregon_snotel_sites.csv')[0] #this is getting the full df of oregon (right now) snotel sites
# #print(type(sites_full))
# station_list = pd.read_csv(path/'stations_of_interest.csv')
# station_list = station_list['oregon'].dropna()
# station_list = station_list.tolist()
# station_list = [str(int(i)) for i in station_list] 
# parameter = 'WTEQ' 
# new_parameter = parameter+'_scaled'
# start_date = "1985-10-01"  
# end_date = "2019-09-30" 
# state = sites_full['state'][0] #this is just looking into the df created from the sites and grabbing the state id. This is used to query the specific station
# change_type='scaled' 
# station = "526:OR:SNTL" 
# #assign some global variables
# #path = Path('/vol/v1/general_files/user_files/ben/')

# #uncomment to run other things- just getting the data that we dont need right now
# #sites_ids = combine.site_list(path/'oregon_snotel_sites.csv')[1] #this is getting a list of just the snotel site ids. You can grab a list slice to restrict how big results gets below. 
# #######################################
# #uncomment when running the full archive 
# sites_full = combine.site_list(path/'oregon_snotel_sites.csv')[0] #this is getting the full df of oregon (right now) snotel sites
# #print(type(sites_full))
# station_list = pd.read_csv(path/'stations_of_interest.csv')
# station_list = station_list['oregon'].dropna()
# station_list = station_list.tolist()
# station_list = [str(int(i)) for i in station_list] 
# parameter = 'WTEQ' 
# new_parameter = parameter+'_scaled'
# start_date = "1985-10-01"  
# end_date = "2019-09-30" 
# state = sites_full['state'][0] #this is just looking into the df created from the sites and grabbing the state id. This is used to query the specific station
# change_type='scaled' 
# station = "526:OR:SNTL" 
###################
#IMPORTANT
###################
#this is currently set up so that pickle_results is the function that is hitting the snotel API. specifying the True/False argument
#dictates whether it pickles the output or just saves in memory. To get a full archive, more sites etc. that line needs to be 
#uncommented. 
#change_type='scaled' 
	#station = "526:OR:SNTL" 
	####################
	#uncomment to run the full archive 

#print(type(sites_full))
	# station_list = pd.read_csv(path/'stations_of_interest.csv')
	# station_list = station_list['oregon'].dropna()
	# station_list = station_list.tolist()
	# station_list = [str(int(i)) for i in station_list] 


	#print (len(results))
		# self.station_csv = station_csv
  #       self.station = station
  #       self.parameter = parameter
  #       self.start_date = start_date
  #       self.end_date = end_date
  #       self.state = state
  #       self.site_list = site_list
  #       self.write_out = write_out#False,1,path,f'{state}_snotel_data_list_1')   
	#sites_full = input_data.make_site_list()[0] #this is getting the full df of oregon (right now) snotel sites
	#print(type(sites_full))
	# station_list = pd.read_csv(path/'stations_of_interest.csv')
	# station_list = station_list['oregon'].dropna()
	# station_list = station_list.tolist()
	# station_list = [str(int(i)) for i in station_list] 
	# parameter = 'WTEQ' 
	# #new_parameter = parameter+'_scaled'
	# start_date = "1985-10-01"  
	# end_date = "2019-09-30" 
	# state = sites_full['state'][0] #this is just looking into the df created from the sites and grabbing the state id. This is used to query the specific station
	#change_type='scaled' 
	#station = "526:OR:SNTL" 

# def site_list(csv_in): 
#     """Get a list of all the snotel sites from input csv."""
#     sites=pd.read_csv(csv_in) #read in the list of snotel sites by state
#     try: 
#         sites['site num']=sites['site name'].str.extract(r"\((.*?)\)") #strip the site numbers from the column with the name and number
#         site_ls= sites['site num'].tolist()
#         print('try')
#         #print(site_ls)
#     except KeyError:  #this was coming from the space instead of _. Catch it and move on. 
#         sites['site num']=sites['site_name'].str.extract(r"\((.*?)\)") #strip the site numbers from the column with the name and number
#         site_ls= sites['site num'].tolist()
#         print('except')
    
#     return (sites,site_ls)

# def get_snotel_data(station, parameter,start_date,end_date): #create a function that pulls down snotel data
#     """Collect snotel data from NRCS API. The guts of the following code block comes from: 
#     https://pypi.org/project/climata/. It is a Python library called climata that was developed to pull down time series 
#     data from various government-maintained networks."""

#     data = StationDailyDataIO(
#         station=station, #station id
#         start_date=start_date, 
#         end_date=end_date,
#         parameter=parameter #assign parameter- need to double check the one for swe
#     )
#     #Display site information and time series data

#     for series in data: 
#         snow_var=pd.DataFrame([row.value for row in series.data]) #create a dataframe of the variable of interest
#         date=pd.DataFrame([row.date for row in series.data]) #get a dataframe of dates
#     df=pd.concat([date,snow_var],axis=1) #concat the dataframes
#     df.columns=['date',f'{parameter}'] #rename columns
#     df['year'] = pd.DatetimeIndex(df['date']).year
#     df['month'] = pd.DatetimeIndex(df['date']).month
#     df['day'] = pd.DatetimeIndex(df['date']).day
#     #df['month'] = df['month'].astype(np.int64)
#     df['id']=station.partition(':')[0] #create an id column from input station 
#     return df  

# def snotel_compiler(sites,state,parameter,start_date,end_date,write_out,version):
#     """Create a list of dataframes. Each df contains the info for one station in a given state. It is intended to put all 
#     of the data from one state into one object."""
#     df_ls = []
#     missing = []
#     count = 0
#     for i in sites: 
#         try: 
#             df=get_snotel_data(f'{i}:{state}:SNTL',f'{parameter}',f'{start_date}',f'{end_date}')
#             df_ls.append(df)
#         except UnboundLocalError as error: 
#             print(f'{i} station data is missing')
#             missing.append(i) 
#             continue
#         count +=1
#         print(count)
#     if write_out: 
#         print('the len of df_ls is: ' + str(len(df_ls)))
#         pickle_data=pickle.dump(df_ls, open( f'{state}_{parameter}_snotel_data_list_{version}', 'ab' ))
#         print('left if statement')
#     else: 
#         print('went to else statement')
#         return df_ls
#     return pickle_data

# def pickle_opener(version,state,filepath,filename): 
#     """If the 'True' argument is specified for snotel_compiler you need this function to read that pickled
#     object back in."""
#     df_ls = pickle.load(open(filepath/filename,'rb'))#pickle.load( open( filepath/f'{state}_snotel_data_list_{version}', 'rb' ) )
#     return df_ls


# def water_years(input_df,start_date,end_date): 
#     """Cut dataframes into water years. The output of this function is a list of dataframes with each dataframe
#     representing a year of data for a single station. """

#     df_ls=[]
#     df_dict={}

#     for year in range(int(start_date[0:4])+1,int(end_date[0:4])): #loop through years
#         #df_dict={}
#         #convert starting and ending dates to datetime objects for slicing the data up by water year
#         startdate = pd.to_datetime(f'{year-1}-10-01').date()
#         enddate = pd.to_datetime(f'{year}-09-30').date()
#         inter = input_df.set_index(['date']) #this is kind of a dumb addition, I am sure there is a better way to do this
#         wy=inter.loc[startdate:enddate] #slice the water year
#         wy.reset_index(inplace=True)#make the index the index again
#         df_dict.update({str(year):wy})
#         df_ls.append(df_dict) #append the dicts to a list
        
#     return df_dict
